# Python Data Science
# ğŸ Python for Data Science â€“ All 15 Tasks

> **Author:** [Pingali TirumalaDatta Sai Parasuram]  
> **Submission:** Python for Data Science â€“ Level 1 through Advanced/GenAI  
> **Tools:** Python 3.10+, pandas, scikit-learn, matplotlib, seaborn, FastAPI

---

## ğŸ“ Project Structure

```
python-data-science/
â”‚
â”œâ”€â”€ tasks/
â”‚   â”œâ”€â”€ task1_data_cleaning.py          # CSV cleaning: nulls, wrong values, dates
â”‚   â”œâ”€â”€ task2_salary_analyzer.py        # Employee salary statistics & report
â”‚   â”œâ”€â”€ task3_utils.py                  # Reusable utility library
â”‚   â”œâ”€â”€ task4_sales_dashboard.py        # Sales analysis + charts
â”‚   â”œâ”€â”€ task5_eda.py                    # Exploratory Data Analysis
â”‚   â”œâ”€â”€ task6_scraper.py                # Web scraper (jobs/products)
â”‚   â”œâ”€â”€ task7_regression.py             # Regression: Linear vs Random Forest
â”‚   â”œâ”€â”€ task8_classification.py         # Classification: Logistic/DTree/RF
â”‚   â”œâ”€â”€ task9_feature_engineering.py    # Feature engineering challenge
â”‚   â”œâ”€â”€ task10_ml_pipeline.py           # End-to-end sklearn Pipeline + joblib
â”‚   â”œâ”€â”€ task11_api_deployment.py        # FastAPI model serving
â”‚   â”œâ”€â”€ task12_report_generator.py      # Automated Excel/PDF reports
â”‚   â”œâ”€â”€ task13_text_analytics.py        # Sentiment analysis + keywords
â”‚   â”œâ”€â”€ task14_chatbot.py               # FAQ chatbot (NLP)
â”‚   â””â”€â”€ task15_data_qa_bot.py           # Natural language CSV Q&A bot
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup & Installation

### 1. Clone the repository
```bash
git clone https://github.com/<your-username>/python-data-science.git
cd python-data-science
```

### 2. Create a virtual environment (recommended)
```bash
python -m venv venv
source venv/bin/activate        # On Windows: venv\Scripts\activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

---

## ğŸš€ Running Each Task

All tasks include built-in sample data, so they run out of the box without any external CSV file.

### Level 1 â€“ Python Foundations

| Task | Command |
|------|---------|
| Task 1 â€“ Data Cleaning | `python tasks/task1_data_cleaning.py --input raw.csv --output cleaned.csv` |
| Task 2 â€“ Salary Analyzer | `python tasks/task2_salary_analyzer.py` |
| Task 3 â€“ Utility Library | `python tasks/task3_utils.py` |

### Data Analysis

| Task | Command |
|------|---------|
| Task 4 â€“ Sales Dashboard | `python tasks/task4_sales_dashboard.py` |
| Task 5 â€“ EDA | `python tasks/task5_eda.py --output eda_output/` |
| Task 6 â€“ Web Scraper | `python tasks/task6_scraper.py --source mock` |

### Machine Learning

| Task | Command |
|------|---------|
| Task 7 â€“ Regression | `python tasks/task7_regression.py` |
| Task 8 â€“ Classification | `python tasks/task8_classification.py` |
| Task 9 â€“ Feature Engineering | `python tasks/task9_feature_engineering.py` |

### Real Industry Projects

| Task | Command |
|------|---------|
| Task 10 â€“ ML Pipeline | `python tasks/task10_ml_pipeline.py --train` |
| Task 11 â€“ API Deployment | `python tasks/task11_api_deployment.py` |
| Task 12 â€“ Report Generator | `python tasks/task12_report_generator.py --format both` |

### Advanced / GenAI

| Task | Command |
|------|---------|
| Task 13 â€“ Text Analytics | `python tasks/task13_text_analytics.py` |
| Task 14 â€“ Chatbot | `python tasks/task14_chatbot.py --demo` |
| Task 15 â€“ Data Q&A Bot | `python tasks/task15_data_qa_bot.py --demo` |

---

## ğŸ“Š Task Descriptions

### Task 1 â€“ Data Cleaning Script
- Removes null rows, fills numeric nulls with median, fills string nulls with "Unknown"
- Detects and fixes negative values in columns like `age`, `salary`, `price`
- Standardizes all date columns to `YYYY-MM-DD` format
- Removes duplicate rows
- Prints a cleaning summary report

### Task 2 â€“ Employee Salary Analyzer
- Computes overall salary stats: average, median, min, max, std deviation
- Groups by department to show department-wise breakdown
- Identifies top 3 highest and lowest paid employees
- Shows salary bracket distribution with ASCII bar chart

### Task 3 â€“ Utility Library
- `remove_duplicates()`: Preserves order, supports `key=` and `case_sensitive=` args
- `normalize_text()`: Removes accents, punctuation, stopwords; case normalizes
- `calculate_zscore()`: Returns Z-scores + optional outlier detection
- `date_formatter()`: Parses 15+ date formats â†’ outputs any target format

### Task 4 â€“ Sales Data Dashboard
- Monthly revenue trend line chart
- Top 5 products by revenue (horizontal bar chart)
- Region-wise revenue distribution (pie chart)
- Exports PNG dashboard + summary CSVs

### Task 5 â€“ EDA
- Summary statistics (mean, std, quartiles, skewness, kurtosis)
- Missing values report with percentages
- Correlation heatmap
- Outlier detection using IQR method
- Saves: distribution plots, correlation heatmap, boxplots, categorical charts

### Task 6 â€“ Web Scraper
- Scrapes job listings from Remotive API (falls back to mock data)
- HTML scraping demo using BeautifulSoup
- Stores results in CSV
- Analyzes: top job titles, locations, in-demand skills

### Task 7 â€“ Regression
- Trains 4 models: Linear Regression, Ridge, Random Forest, Gradient Boosting
- Metrics: RMSE, RÂ², MAE, MAPE, 5-fold CV
- Feature importance chart
- Saves best model as `.pkl` via joblib

### Task 8 â€“ Classification
- Trains: Logistic Regression, Decision Tree, Random Forest
- Metrics: Accuracy, Precision, Recall, F1, AUC-ROC
- Confusion matrices + ROC curves + metric comparison bar chart

### Task 9 â€“ Feature Engineering
- Creates: debt-to-income ratio, income per age, risk score composite
- Bins age groups, flags high loan amounts
- One-hot encodes categoricals, polynomial interaction features
- Demonstrates 10%+ F1 improvement over baseline

### Task 10 â€“ End-to-End ML Pipeline
- sklearn ColumnTransformer for numeric + categorical features
- Handles imputation, scaling, encoding in a single Pipeline
- Saves trained pipeline to `.pkl` with joblib
- `--predict` flag for loading saved model and making predictions

### Task 11 â€“ API Deployment (FastAPI)
- `GET /health` â†’ API health check
- `POST /predict` â†’ single prediction with probability + risk level
- `POST /predict/batch` â†’ batch predictions
- Interactive Swagger docs at `http://localhost:8000/docs`
- Install: `pip install fastapi uvicorn`

### Task 12 â€“ Automated Report Generator
- Reads any sales CSV and computes KPIs automatically
- Exports multi-sheet Excel report (`openpyxl`)
- Exports multi-page PDF with charts (`matplotlib`)
- Outputs: KPI overview, monthly sales, region performance

### Task 13 â€“ Text Analytics
- Rule-based + lexicon sentiment analysis (positive/negative/neutral)
- Handles negation ("not great" â†’ negative)
- Extracts top keywords using frequency analysis
- ASCII word cloud visualization

### Task 14 â€“ FAQ Chatbot
- Custom TF-IDF-like similarity for FAQ matching
- 15+ data science Q&A pairs in knowledge base
- Interactive and demo modes
- Easily extensible knowledge base

### Task 15 â€“ Data Q&A Bot
- Natural language interface to any CSV/DataFrame
- Handles: aggregations, groupby, filtering, top-N, describe
- Example: "Show top 3 months by revenue" â†’ groupby + nlargest
- Interactive and demo modes

---

## ğŸ”§ Requirements

```
pandas>=2.0
numpy>=1.24
matplotlib>=3.7
seaborn>=0.12
scikit-learn>=1.3
joblib>=1.3
requests>=2.31
beautifulsoup4>=4.12
openpyxl>=3.1
fastapi>=0.104          # Task 11 only
uvicorn>=0.24           # Task 11 only
```

---

## ğŸ“¸ Screenshots

> Screenshots of outputs are located in the `/screenshots` folder of this repository.

---

## ğŸ“ Submission Notes

- All 15 tasks are fully functional and include built-in sample data
- Each task can run standalone with `python tasks/taskN_*.py`
- Clean, PEP8-compliant code with docstrings and type hints
- Tasks 7â€“11 save models and outputs to disk
- See the video walkthrough for a live demonstration of all tasks

---

## ğŸ“„ License

MIT License â€” free to use and modify.
